# -*- coding: utf-8 -*-
"""AI Engg Assessment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19UPE3xkdEywWR4v3UW0zZOPCfxduACj8
"""

!pip install torch transformers gradio openai-whisper
!pip install transformers==4.35.2
!pip install flash-attn --no-build-isolation
!pip install torch --index-url https://download.pytorch.org/whl/cu121
!pip install --upgrade transformers
!pip install TTS
!pip install pyttsx3
!pip install gtts
!pip install IPython

from huggingface_hub import login
login()

import gradio as gr
import torch
import json
import sqlite3
import librosa
import soundfile as sf
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from functools import lru_cache
import whisper

# Load model and tokenizer
MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.3"
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, torch_dtype=torch.float16, device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# Load Whisper for STT
whisper_model = whisper.load_model("base")


# Load training prompts from config file
def load_prompts():
    prompt_file = "/content/training_prompts.json"  # Update path for Colab
    try:
        with open(prompt_file, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"Error: Could not find {prompt_file}. Ensure the file is uploaded to Colab.")
        return {}  # Return an empty dictionary if file is missing


training_prompts = load_prompts()

# Setup SQLite database for tracking progress
def setup_db():
    conn = sqlite3.connect("progress.db")
    c = conn.cursor()
    c.execute(
        """CREATE TABLE IF NOT EXISTS progress
           (user TEXT, task TEXT, score INTEGER, feedback TEXT)"""
    )
    conn.commit()
    conn.close()


setup_db()


# Function to track progress
def log_progress(user, task, score, feedback):
    conn = sqlite3.connect("progress.db")
    c = conn.cursor()
    c.execute("INSERT INTO progress VALUES (?, ?, ?, ?)", (user, task, score, feedback))
    conn.commit()
    conn.close()


# Optimized response generation
@lru_cache(maxsize=100)
def generate_feedback(prompt):
    """Generates structured feedback with optimized parameters for a T4 GPU."""
    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,  # Limit response length
            temperature=0.7,  # Keeps responses coherent yet creative
            top_p=0.9,  # Controls randomness
            top_k=50,  # Diverse but logical choices
            repetition_penalty=1.05,  # Reduces repetition
            do_sample=True,  # Enables natural variation
        )
        return tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        print("Error during response generation:", str(e))
        return "Error generating feedback."


# Preprocess audio (noise reduction)
def preprocess_audio(file_path):
    y, sr = librosa.load(file_path, sr=16000)
    y = librosa.effects.preemphasis(y)
    sf.write(file_path, y, sr)
    return file_path


# Batch inference for efficiency
def batch_generate_feedback(prompts):
    inputs = tokenizer(prompts, return_tensors="pt", padding=True).to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        temperature=0.7,
        top_p=0.9,
        top_k=50,
        repetition_penalty=1.05,
        do_sample=True,
    )
    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]


# Unload model when idle
def unload_model():
    global model
    model = None
    torch.cuda.empty_cache()


# Gradio UI
def evaluate_speech(audio_file):
    processed_audio = preprocess_audio(audio_file)
    text = whisper_model.transcribe(processed_audio)["text"].strip()

    if not text:
        return "No speech detected in the audio."

    return generate_feedback(f"Evaluate this speech: {text}")


def evaluate_text(text):
    return generate_feedback(f"Evaluate this speech: {text}")


def train_skill(skill, user):
    prompt = training_prompts.get(skill, "Describe the importance of communication skills.")
    feedback = generate_feedback(prompt)
    score = np.random.randint(1, 10)  # Mock score
    log_progress(user, skill, score, feedback)
    return feedback


def get_progress(user):
    conn = sqlite3.connect("progress.db")
    c = conn.cursor()
    c.execute("SELECT task, score, feedback FROM progress WHERE user=?", (user,))
    data = c.fetchall()
    conn.close()
    return data if data else "No progress recorded."


# **Gradio UI**
with gr.Blocks() as demo:
    gr.Markdown("## Verbal Communication Skills Trainer")

    with gr.Tab("Chat Feedback"):
        text_input = gr.Textbox(label="Enter text for feedback")
        submit_text = gr.Button("Submit")
        output_text = gr.Textbox()
        submit_text.click(evaluate_text, inputs=text_input, outputs=output_text)

    with gr.Tab("Voice Feedback"):
        audio_input = gr.Audio(type="filepath", label="Upload Audio")  # Fixed 'source' issue
        submit_audio = gr.Button("Submit")
        output_audio = gr.Textbox()
        submit_audio.click(evaluate_speech, inputs=audio_input, outputs=output_audio)

    with gr.Tab("Training"):
        skill_choice = gr.Dropdown(
            choices=["Impromptu Speaking", "Storytelling", "Conflict Resolution"],
            label="Select Skill",
        )
        user_input = gr.Textbox(label="Enter Your Name")
        train_button = gr.Button("Train")
        train_output = gr.Textbox()
        train_button.click(train_skill, inputs=[skill_choice, user_input], outputs=train_output)

    with gr.Tab("Progress Tracking"):
        user_query = gr.Textbox(label="Enter Your Name")
        progress_button = gr.Button("Get Progress")
        progress_output = gr.Textbox()
        progress_button.click(get_progress, inputs=user_query, outputs=progress_output)

demo.launch()